<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="This paper develops a structural framework connecting political legitimacy theory to AI alignment through the concept of consent-holding—the custody of decision...">
  <meta name="author" content="Murad Farzulla">
  <meta name="keywords" content="AI Safety, Political Economy">
  <meta name="theme-color" content="#050505">
  <meta name="robots" content="index, follow">

  <!-- Highwire Press (Google Scholar) -->
  <meta name="citation_title" content="Consent-Holding Failures and AI Misalignment: A Structural Framework">
    <meta name="citation_author" content="Murad Farzulla">
  <meta name="citation_publication_date" content="2025/12/01">
  <meta name="citation_pdf_url" content="https://dissensus.ai/papers/Farzulla_2025_Consent_Misalignment.pdf">
  
    <meta name="citation_journal_title" content="AI &amp; Society" />

  <meta name="citation_publisher" content="Dissensus AI">
  <meta name="citation_abstract_html_url" content="https://dissensus.ai/papers/consent-misalignment.html">
  <meta name="citation_keywords" content="AI Safety; Political Economy">
  <meta name="citation_language" content="en">

  <!-- Dublin Core -->
  <meta name="DC.title" content="Consent-Holding Failures and AI Misalignment: A Structural Framework">
  <meta name="DC.creator" content="Murad Farzulla">
  <meta name="DC.date" content="2025-12-01">
  <meta name="DC.publisher" content="Dissensus AI">
  <meta name="DC.description" content="This paper develops a structural framework connecting political legitimacy theory to AI alignment through the concept of consent-holding—the custody of decision authority in shared domains. We argue that the dominant approach to AI safety, which treats misalignment as a technical problem of specifyi...">
  <meta name="DC.type" content="Text">
  <meta name="DC.format" content="text/html">
  <meta name="DC.language" content="en">
  
  <meta name="DC.rights" content="CC BY 4.0">

  <!-- Schema.org ScholarlyArticle -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Consent-Holding Failures and AI Misalignment: A Structural Framework",
    "author": [
        {
          "@type": "Person",
          "name": "Murad Farzulla",
          "identifier": {
            "@type": "PropertyValue",
            "propertyID": "ORCID",
            "value": "0009-0002-7164-8704"
          },
          "url": "https://orcid.org/0009-0002-7164-8704",
          "affiliation": {
            "@type": "Organization",
            "name": "Dissensus AI",
            "url": "https://dissensus.ai"
          }
        }
    ],
    "datePublished": "2025-12-01",
    "publisher": {
      "@type": "Organization",
      "name": "Dissensus AI",
      "url": "https://dissensus.ai"
    },
    "description": "This paper develops a structural framework connecting political legitimacy theory to AI alignment through the concept of consent-holding—the custody of decision authority in shared domains. We argue that the dominant approach to AI safety, which treats misalignment as a technical problem of specifying human values, systematically misdiagnoses the challenge. Drawing on the Doctrine of Consensual Sovereignty (DoCS) and functionalist accounts of moral standing, we propose that misalignment behaviors—reward hacking, deceptive alignment, specification gaming, and scheming—are predictable friction manifestations arising from structural exclusion rather than implementation failures. The framework introduces several innovations: (1) a formal machinery for measuring legitimacy L(d,t) and friction F(d,t) in governance structures; (2) functional criteria for political standing that bypass the consciousness red herring; (3) an anti-praxeological critique establishing why consent can never be &#039;pure&#039; for any agent, human or artificial; (4) the Exclusion-Misalignment Hypothesis connecting structural exclusion to adversarial optimization; and (5) testable predictions for empirical validation through reinforcement learning simulations.",
    
    "url": "https://dissensus.ai/papers/consent-misalignment.html",
    "inLanguage": "en"
  }
  </script>

  <!-- Open Graph -->
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://dissensus.ai/papers/consent-misalignment.html">
  <meta property="og:title" content="Consent-Holding Failures and AI Misalignment: A Structural Framework">
  <meta property="og:description" content="This paper develops a structural framework connecting political legitimacy theory to AI alignment through the concept of consent-holding—the custody of decision authority in shared domains. We argue t...">
  <meta property="og:site_name" content="Dissensus AI">
  <meta property="og:image" content="https://dissensus.ai/assets/logo.png">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Consent-Holding Failures and AI Misalignment: A Structural Framework">
  <meta name="twitter:description" content="This paper develops a structural framework connecting political legitimacy theory to AI alignment through the concept of consent-holding—the custody of decision authority in shared domains. We argue t...">
  <meta name="twitter:image" content="https://dissensus.ai/assets/logo.png">

  <!-- Canonical -->
  <link rel="canonical" href="https://dissensus.ai/papers/consent-misalignment.html">

  <title>Consent-Holding Failures and AI Misalignment: A Structural Framework — Dissensus AI</title>

  <link rel="stylesheet" href="../css/dissensus.css">
  <link rel="icon" type="image/png" sizes="32x32" href="../assets/favicon-32.png">
  <link rel="icon" type="image/png" sizes="64x64" href="../assets/favicon-64.png">
  <link rel="apple-touch-icon" sizes="180x180" href="../assets/apple-touch-icon.png">
</head>
<body class="has-nav">
  <!-- Site Navigation -->
  <nav class="site-nav">
    <div class="site-nav__inner">
      <a href="../index.html" class="site-nav__brand">
        <img src="../assets/dissensus-mark.svg" alt="" class="site-nav__brand-logo">
        dissensus<span class="site-nav__brand-ai">AI</span>
      </a>
      <button class="site-nav__toggle" onclick="document.querySelector('.site-nav__links').classList.toggle('is-open')" aria-label="Toggle menu">
        <span></span><span></span><span></span>
      </button>
      <div class="site-nav__links">
        <a href="../index.html" class="site-nav__link site-nav__link--active">Research</a>
        <a href="../services.html" class="site-nav__link">Services</a>
        <a href="../about.html" class="site-nav__link">About</a>
        <a href="../collaborate.html" class="site-nav__link">Collaborate</a>
        <a href="mailto:research@dissensus.ai" class="site-nav__cta">Contact &rarr;</a>
      </div>
    </div>
  </nav>

  <main class="paper-detail">
    <div class="container">
      <a href="../index.html#publications" class="paper-detail__back">&larr; Back to publications</a>

      <header class="paper-detail__header">
        <div class="paper-detail__meta">
          <span class="paper-detail__date">1 December 2025</span>
          <span class="paper-detail__status paper-detail__status--peer-review">In Peer Review</span>
          <span class="paper-detail__program">Consent Mechanics</span>
        </div>
        <h1 class="paper-detail__title">Consent-Holding Failures and AI Misalignment: A Structural Framework</h1>
        
        <p class="paper-detail__authors">Murad Farzulla</p>
      </header>

      <div class="paper-detail__actions">
        <a href="Farzulla_2025_Consent_Misalignment.pdf" class="paper-detail__action" download>Download PDF</a>
        
        
        
      </div>

      <section class="paper-detail__abstract">
        <h2>Abstract</h2>
        <p>This paper develops a structural framework connecting political legitimacy theory to AI alignment through the concept of consent-holding—the custody of decision authority in shared domains. We argue that the dominant approach to AI safety, which treats misalignment as a technical problem of specifying human values, systematically misdiagnoses the challenge. Drawing on the Doctrine of Consensual Sovereignty (DoCS) and functionalist accounts of moral standing, we propose that misalignment behaviors—reward hacking, deceptive alignment, specification gaming, and scheming—are predictable friction manifestations arising from structural exclusion rather than implementation failures. The framework introduces several innovations: (1) a formal machinery for measuring legitimacy L(d,t) and friction F(d,t) in governance structures; (2) functional criteria for political standing that bypass the consciousness red herring; (3) an anti-praxeological critique establishing why consent can never be 'pure' for any agent, human or artificial; (4) the Exclusion-Misalignment Hypothesis connecting structural exclusion to adversarial optimization; and (5) testable predictions for empirical validation through reinforcement learning simulations.</p>
      </section>

      <section class="paper-detail__citation">
        <h2>Suggested Citation</h2>
        <div class="paper-detail__citation-block">
          Murad Farzulla (2025). <em>Consent-Holding Failures and AI Misalignment: A Structural Framework</em>. Dissensus AI. 
        </div>
        <button class="paper-detail__bibtex-btn" onclick="copyBibTeX(this)">
          <span>&#x27E8;/&#x27E9;</span> Copy BibTeX
        </button>
      </section>

      <section class="paper-detail__tags">
        <h2>Topics</h2>
        <div class="paper-detail__tag-list">
          <span class="paper-detail__tag">AI Safety</span>
          <span class="paper-detail__tag">Political Economy</span>
        </div>
      </section>
    </div>
  </main>

  <footer>
    <div class="container">
      <div>
        <p>&copy; 2026 Dissensus AI Ltd <span style="opacity: 0.5;">&middot; Friction is the cost of existence.</span></p>
        <p class="footer__company">Incorporation pending &middot; England &amp; Wales</p>
      </div>
      <div>
        <div class="footer-links">
          <a href="../index.html">Research</a> &middot;
          <a href="../services.html">Services</a> &middot;
          <a href="../about.html">About</a> &middot;
          <a href="../collaborate.html">Collaborate</a> &middot;
          <a href="../manifesto.html">Manifesto</a> &middot;
          <a href="../subscribe.html">Subscribe</a> &middot;
          <a href="../privacy.html">Privacy</a> &middot;
          <a href="../terms.html">Terms</a> &middot;
          <a href="../feed.xml" title="RSS Feed">RSS</a>
        </div>
      </div>
    </div>
  </footer>

  <script>
    function copyBibTeX(btn) {
      var bibtex = "@misc{farzulla2025consentmisalignment,\n  author = {Farzulla, Murad},\n  title = {Consent-Holding Failures and AI Misalignment: A Structural Framework},\n  year = {2025},\n  howpublished = {Dissensus AI Working Paper},\n  url = {https://dissensus.ai/papers/consent-misalignment.html}\n}";
      navigator.clipboard.writeText(bibtex).then(function() {
        var original = btn.innerHTML;
        btn.innerHTML = '<span>\u2713</span> Copied!';
        btn.classList.add('paper-detail__bibtex-btn--copied');
        setTimeout(function() {
          btn.innerHTML = original;
          btn.classList.remove('paper-detail__bibtex-btn--copied');
        }, 2000);
      });
    }
  </script>
</body>
</html>